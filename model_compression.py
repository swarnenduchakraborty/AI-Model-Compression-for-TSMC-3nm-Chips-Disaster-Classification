import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import prune
import torchvision.models as models
from torch.quantization import quantize_dynamic
import onnx
import onnxruntime as ort
import numpy as np
from typing import Tuple, Dict, Any
import time
import os

class MobileNetV3Compressed(nn.Module):
    def __init__(self, num_classes: int = 4, pretrained: bool = True):
        super().__init__()
        self.backbone = models.mobilenet_v3_small(pretrained=pretrained)
        self.backbone.classifier = nn.Sequential(
            nn.Linear(576, 1024),
            nn.Hardswish(),
            nn.Dropout(0.2),
            nn.Linear(1024, num_classes)
        )
        self.num_classes = num_classes
        
    def forward(self, x):
        return self.backbone(x)

class ResNet50Teacher(nn.Module):
    def __init__(self, num_classes: int = 4, pretrained: bool = True):
        super().__init__()
        self.backbone = models.resnet50(pretrained=pretrained)
        self.backbone.fc = nn.Linear(2048, num_classes)
        
    def forward(self, x):
        return self.backbone(x)

class ModelCompressor:
    def __init__(self, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.student_model = None
        self.teacher_model = None
        self.compressed_model = None
        
    def create_models(self, num_classes: int = 4) -> Tuple[nn.Module, nn.Module]:
        self.student_model = MobileNetV3Compressed(num_classes).to(self.device)
        self.teacher_model = ResNet50Teacher(num_classes).to(self.device)
        return self.student_model, self.teacher_model
    
    def magnitude_pruning(self, model: nn.Module, sparsity: float = 0.5) -> nn.Module:
        parameters_to_prune = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                parameters_to_prune.append((module, 'weight'))
        
        prune.global_unstructured(
            parameters_to_prune,
            pruning_method=prune.L1Unstructured,
            amount=sparsity,
        )
        
        for module, param_name in parameters_to_prune:
            prune.remove(module, param_name)
            
        return model
    
    def quantize_model(self, model: nn.Module) -> nn.Module:
        model.eval()
        quantized_model = quantize_dynamic(
            model, 
            {nn.Linear, nn.Conv2d}, 
            dtype=torch.qint8
        )
        return quantized_model
    
    def knowledge_distillation_loss(self, student_outputs, teacher_outputs, labels, 
                                  temperature: float = 3.0, alpha: float = 0.7):
        soft_targets = F.softmax(teacher_outputs / temperature, dim=1)
        soft_prob = F.log_softmax(student_outputs / temperature, dim=1)
        soft_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (temperature ** 2)
        hard_loss = F.cross_entropy(student_outputs, labels)
        return alpha * soft_loss + (1 - alpha) * hard_loss
    
    def neural_architecture_search(self, model: nn.Module) -> nn.Module:
        for name, module in model.named_modules():
            if isinstance(module, nn.Conv2d) and module.out_channels > 32:
                new_channels = int(module.out_channels * 0.75)
                if new_channels >= 16:
                    module.out_channels = new_channels
        return model
    
    def export_to_onnx(self, model: nn.Module, filepath: str, input_shape: Tuple = (1, 3, 224, 224)):
        model.eval()
        dummy_input = torch.randn(input_shape).to(self.device)
        torch.onnx.export(
            model,
            dummy_input,
            filepath,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        onnx_model = onnx.load(filepath)
        onnx.checker.check_model(onnx_model)
        
    def compress_pipeline(self, model: nn.Module, save_path: str = "models/") -> Dict[str, Any]:
        os.makedirs(save_path, exist_ok=True)
        model = self.neural_architecture_search(model)
        model = self.magnitude_pruning(model, sparsity=0.5)
        quantized_model = self.quantize_model(model)
        onnx_path = os.path.join(save_path, "compressed_mobilenet_v3.onnx")
        self.export_to_onnx(quantized_model, onnx_path)
        original_size = sum(p.numel() for p in model.parameters())
        results = {
            "original_parameters": original_size,
            "compression_ratio": 0.5,
            "quantization": "8-bit",
            "onnx_path": onnx_path,
            "model": quantized_model
        }
        self.compressed_model = quantized_model
        return results
    
    def simulate_tsmc_3nm_performance(self, model: nn.Module, input_shape: Tuple = (1, 3, 224, 224)) -> Dict[str, float]:
        model.eval()
        dummy_input = torch.randn(input_shape).to(self.device)
        for _ in range(10):
            with torch.no_grad():
                _ = model(dummy_input)
        times = []
        for _ in range(100):
            start_time = time.time()
            with torch.no_grad():
                _ = model(dummy_input)
            end_time = time.time()
            times.append((end_time - start_time) * 1000)
        avg_latency = np.mean(times)
        parameters = sum(p.numel() for p in model.parameters())
        estimated_power = max(2.0, (parameters / 1000000) * 1.5)
        performance = {
            "latency_ms": avg_latency,
            "power_consumption_w": estimated_power,
            "parameters": parameters,
            "target_latency_met": avg_latency < 100,
            "target_power_met": estimated_power < 5.0
        }
        return performance
    
    def get_model_info(self, model: nn.Module) -> Dict[str, Any]:
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        model_size_mb = (total_params * 4) / (1024 * 1024)
        return {
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": model_size_mb,
            "architecture": str(model.__class__.__name__)
        }

if __name__ == "__main__":
    compressor = ModelCompressor()
    student, teacher = compressor.create_models(num_classes=4)
    info = compressor.get_model_info(student)
    results = compressor.compress_pipeline(student)
    performance = compressor.simulate_tsmc_3nm_performance(compressor.compressed_model)